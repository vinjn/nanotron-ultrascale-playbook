# LLM面试常用代码（自写，仅参考）

**Author:** 浮生梦晓

**Date:** 2025-05-05

**Link:** https://zhuanlan.zhihu.com/p/1900262435190964867

## [多头注意力](https://zhida.zhihu.com/search?content_id=257115381&content_type=Article&match_order=1&q=%E5%A4%9A%E5%A4%B4%E6%B3%A8%E6%84%8F%E5%8A%9B&zhida_source=entity)——MHA

```text
import torch
from torch import nn
import math
torch.manual_seed(1234)
# input_size = (bs, seq_len, d_model)
batch_size = 4
seq_len = 128
d_model = 64
num_heads = 8


class MultiHeadAttention(nn.Module):
    def __init__(self):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = d_model // num_heads
        self.q_proj = nn.Linear(d_model, d_model, bias=False)
        self.k_proj = nn.Linear(d_model, d_model, bias=False)
        self.v_proj = nn.Linear(d_model, d_model, bias=False)
        self.out_proj = nn.Linear(d_model, d_model, bias=False)

    def forward(self, x):
        bs, seq_len, d_model = x.size()
        q = self.q_proj(x).view(bs, seq_len, self.num_heads, self.head_dim).transpose(1, 2)
        k = self.k_proj(x).view(bs, seq_len, self.num_heads, self.head_dim).transpose(1, 2)
        v = self.v_proj(x).view(bs, seq_len, self.num_heads, self.head_dim).transpose(1, 2)

        # Compute attention scores
        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.head_dim)

        # Apply mask
        mask = torch.tril(torch.ones(seq_len, seq_len)).to(x.device)
        scores = scores.masked_fill(mask == 0, float("-inf"))

        # Apply softmax
        attn_weights = nn.functional.softmax(scores, dim=-1)

        # Compute output
        out = torch.matmul(attn_weights, v)
        out = out.transpose(1, 2).contiguous().view(bs, seq_len, -1)
        out = self.out_proj(out)
        return out


x = torch.randn(batch_size, seq_len, d_model)
multi_head_attention = MultiHeadAttention()
out = multi_head_attention(x)
print(out.shape)
```

## [分组注意力](https://zhida.zhihu.com/search?content_id=257115381&content_type=Article&match_order=1&q=%E5%88%86%E7%BB%84%E6%B3%A8%E6%84%8F%E5%8A%9B&zhida_source=entity)——GQA

```text
import torch
from torch import nn
import math

torch.manual_seed(1234)
# input_size = (bs, seq_len, d_model)
batch_size = 4
seq_len = 128
d_model = 64
num_heads = 8
# 定义键值头的数量
num_kv_heads = 2


class GroupQueryAttention(nn.Module):
    def __init__(self):
        super().__init__()
        self.num_heads = num_heads
        self.num_kv_heads = num_kv_heads
        self.head_dim = d_model // num_heads
        self.q_proj = nn.Linear(d_model, d_model, bias=False)
        self.k_proj = nn.Linear(d_model, num_kv_heads * self.head_dim, bias=False)
        self.v_proj = nn.Linear(d_model, num_kv_heads * self.head_dim, bias=False)
        self.out_proj = nn.Linear(d_model, d_model, bias=False)

    def forward(self, x):
        bs, seq_len, d_model = x.size()
        q = self.q_proj(x).view(bs, seq_len, self.num_heads, self.head_dim).transpose(1, 2)
        k = self.k_proj(x).view(bs, seq_len, self.num_kv_heads, self.head_dim).transpose(1, 2)
        v = self.v_proj(x).view(bs, seq_len, self.num_kv_heads, self.head_dim).transpose(1, 2)

        # 复制键值头以匹配查询头的数量
        k = k.repeat(1, self.num_heads // self.num_kv_heads, 1, 1)
        v = v.repeat(1, self.num_heads // self.num_kv_heads, 1, 1)

        # 计算注意力分数
        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.head_dim)

        # 应用掩码
        mask = torch.tril(torch.ones(seq_len, seq_len)).to(x.device)
        scores = scores.masked_fill(mask == 0, float("-inf"))

        # 应用softmax
        attn_weights = nn.functional.softmax(scores, dim=-1)

        # 计算输出
        out = torch.matmul(attn_weights, v)
        out = out.transpose(1, 2).contiguous().view(bs, seq_len, -1)
        out = self.out_proj(out)
        return out


x = torch.randn(batch_size, seq_len, d_model)
group_query_attention = GroupQueryAttention()
out = group_query_attention(x)
print(out.shape)
    
```

## 模型训练——[Pytorch](https://zhida.zhihu.com/search?content_id=257115381&content_type=Article&match_order=1&q=Pytorch&zhida_source=entity)

```text
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import matplotlib.pyplot as plt

# 生成一些示例数据
np.random.seed(42)
x = np.linspace(0, 10, 100)
y = 2 * x + 1 + np.random.randn(100) * 1

# 将数据转换为 PyTorch 张量
x_tensor = torch.from_numpy(x).float().view(-1, 1)
y_tensor = torch.from_numpy(y).float().view(-1, 1)

# 定义简单的线性回归模型
class LinearRegression(nn.Module):
    def __init__(self):
        super(LinearRegression, self).__init__()
        self.linear = nn.Linear(1, 1)

    def forward(self, x):
        return self.linear(x)

# 初始化模型、损失函数和优化器
model = LinearRegression()
criterion = nn.MSELoss()
optimizer = optim.SGD(model.parameters(), lr=0.01)

# 训练模型
num_epochs = 1000
for epoch in range(num_epochs):
    # 前向传播
    outputs = model(x_tensor)
    loss = criterion(outputs, y_tensor)

    # 反向传播和优化
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    if (epoch + 1) % 100 == 0:
        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}')

# 绘制结果
predicted = model(x_tensor).detach().numpy()
plt.scatter(x, y, label='Original data')
plt.plot(x, predicted, color='red', label='Fitted line')
plt.legend()
plt.show()
    
```