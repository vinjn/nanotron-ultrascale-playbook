# 深入理解 GPU 优化技巧背后的机制

**Author:** ToyX

**Date:** 2025-07-05

**Link:** https://zhuanlan.zhihu.com/p/1922079603645740239

本篇稍微讨论一下 [GPU Kernel](https://zhida.zhihu.com/search?content_id=259649165&content_type=Article&match_order=1&q=GPU+Kernel&zhida_source=entity) 各项优化技巧背后的机制，为了提升严谨性，会尽量提供参考资料链接，由于 GPU 软硬件迭代迅速，这里仅探讨一些经典话题。笔者刚好在开发算子库 [GitHub - xytpai/kfunca: A minimalist high-performance GPU AI kernel library](https://link.zhihu.com/?target=https%3A//github.com/xytpai/kfunca)，顺带留这篇记录下，持续更新。

## 知识点一览

-   1\. [向量化访存](https://zhida.zhihu.com/search?content_id=259649165&content_type=Article&match_order=1&q=%E5%90%91%E9%87%8F%E5%8C%96%E8%AE%BF%E5%AD%98&zhida_source=entity)与[访存合并](https://zhida.zhihu.com/search?content_id=259649165&content_type=Article&match_order=1&q=%E8%AE%BF%E5%AD%98%E5%90%88%E5%B9%B6&zhida_source=entity) (Vectorize & Coalescing)
-   2\. Occupancy
-   3\. [Tail Effect](https://zhida.zhihu.com/search?content_id=259649165&content_type=Article&match_order=1&q=Tail+Effect&zhida_source=entity)
-   4\. [Reduction 最佳实践](https://zhida.zhihu.com/search?content_id=259649165&content_type=Article&match_order=1&q=Reduction+%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5&zhida_source=entity)： Warp Shuffle
-   5\. 如何手撕 [GEMM](https://zhida.zhihu.com/search?content_id=259649165&content_type=Article&match_order=1&q=GEMM&zhida_source=entity) 跑满 Tensor Core 理论 FLOPS：Tiling & Bank conflict & Swizzle & Tensor-Core & AB Buffer
-   6\. 如何根据 Transformer 大模型的参数量确定 GPU 的各项指标

## 1\. 向量化访存与访存合并 (Vectorize & Coalescing)

我们可以用这么一个 Kernel 做 Copy ，它可以在大量数据下打满 GPU 带宽（首地址必须对齐）：

```cuda
// 对齐化结构体
template <typename T, int vec_size>
struct alignas(sizeof(T) * vec_size) aligned_array {
    T val[vec_size]; // 注意 CUDA 推荐使用 Structure of Arrays (SoA) 风格
};

// 从经验上，一般使用场景是 16B 向量化，比如 T 是 Float 那么 vec_size 是 4
// 注意上述情况下 首地址 必须是 16B 对齐，因为硬件连线是死的(固定内存片段到固定内存片段)
template <typename T, int vec_size>
__global__ void threads_copy_kernel(const T *in, T *out, const size_t n) {
    
    // 每个 Block Copy 的数据个数
    const int block_work_size = blockDim.x * vec_size; 
    
     // 当前 Block 中，当前 Thread 对应的 Vector 首地址
    auto index = blockIdx.x * block_work_size + threadIdx.x * vec_size;
    
    auto remaining = n - index;
    if (remaining < vec_size) { // 这里处理尾巴部分，即 n 不被 vec_size 整除的情况
        for (auto i = index; i < n; i++) { // for loop 拷贝
            out[i] = in[i];
        }
    } else {
        using vec_t = aligned_array<T, vec_size>;
        auto in_vec = reinterpret_cast<vec_t *>(const_cast<T *>(&in[index]));
        auto out_vec = reinterpret_cast<vec_t *>(&out[index]);
        *out_vec = *in_vec; // 这个就是向量化读写
    }
}
```

这里我们可以直接输出 PTX 观察到这两条指令:

```text
st.global.v4.f32
st.global.v4.f32
```

注意这是每个 Thread 要执行的指令（Volta架构后 Warp 中的每个线程都有它的专有PC [Independent Thread Scheduling](https://link.zhihu.com/?target=https%3A//docs.nvidia.com/cuda/volta-tuning-guide/index.html)），它是怎么提升访存效率的？我们先来看一下它的宽度，4\*4B = 16B = 128bit。然后我们来尝试把这个指令对应到硬件上的某个设施上：

![](https://pic2.zhimg.com/v2-3cb423542bd49307142d764f0c5fe0d1_1440w.jpg)

NVIDIA ADA GPU ARCHITECTURE

上图中，LD/ST 就是用来和 GPU Device Memory 交互的 （LSU），并且每个时刻被 Warp Scheduler 处理的 Warp 中，同时可以有 4 个 LSU 设施进行访存。

由于 LSU 是直接经过 L1TEX Data Cache 的，因此，我们可以关注它的位宽。从各种资料来看， L1 每个 Bank 是 4B，那么仅用 Float1 就足够了：一个SLU分到 32/4=8 个 Bank ，这里如果 32 个 Thread 同时写 4B，每个 LSU 可以把 8 个 Thread 捆一起输出 32B 到 8 个 Bank 上；如果同时写 8B，LSU 把 4 个 Thread 捆在一起输出 32B 同样到 8 个 Bank 上，但是总共需要 2 个 Cycle 了，这里每个 LSU 的 32B 访存就是 [Transaction](https://link.zhihu.com/?target=https%3A//docs.nvidia.com/gameworks/content/developertools/desktop/analysis/report/cudaexperiments/sourcelevel/memorytransactions.htm) 的概念：

![](https://picx.zhimg.com/v2-05e2cb06209309be94c400247fb620c9_1440w.jpg)

Transaction

[上图](https://link.zhihu.com/?target=https%3A//docs.nvidia.com/cuda/cuda-c-best-practices-guide/) 就是一个 Warp Scheduler 同时启用 4 个 LSU 的情况，每个 LSU 处理 32B 对齐数据，合在一起就是同时 128B 的传输单元，而这个传输单元，就是 L1 或者 L2 的 CacheLine 大小。我们如果能保证这 128B 是同时进行的，就满足访存合并的要求（_Coalesced access_），这时 4 个 LSU 能把全部数据直接读到 CacheLine 中。当然，如果每个 LSU 取一个随机的 32B 对齐数据块，且不重复，看起来也能同时访问 128B，但实际上由于 CacheLine 需要映射连续的 Device Memory，所以效率不佳。因此最好 **一个 Warp 的 32 个 Thread 访问连续的 128B \* 正整数 的数据量（不建议过大），且数据头指针是 32B 对齐，这样就是 Coalesed Access**。

![](https://pic2.zhimg.com/v2-ed77f3ee46ddd37fc83a95d188774455_1440w.jpg)

从 [https://docs.nvidia.com/nsight-graphics/AdvancedLearning/index.html](https://link.zhihu.com/?target=https%3A//docs.nvidia.com/nsight-graphics/AdvancedLearning/index.html) 中也能看到，8B 或者 16B Access 拆成多个 Cycle。因此，**单纯从 LSU 上看 Float1 足够了，而使用 Float4 的好处就只有通过减少指令数量来减少Request**。（宏观上可能被Hide掉）

![](https://pic1.zhimg.com/v2-4ff9a26a2caa083357628f162fc6ba0c_1440w.jpg)

那如果是 2B 呢？ 2B 的 Throughput 比 4B 低许多，原因是，一个 LSU 必须同时处理多达 16 个 Thread，因此上述架构中有一半 LSU 是浪费的，这也浪费了一半 CacheLine，因此损害了 Throughput。我们做一下 1GB Copy 实验，只有 Half 达不到带宽峰值：

```text
1GB threads copy test ...
float1: timems:2.32653
923.042 GBPS ... ok
float2: timems:2.33245
920.699 GBPS ... ok
float4: timems:2.33267
920.611 GBPS ... ok
float8: timems:2.3311
921.23 GBPS ... ok
half: timems:2.66342
806.287 GBPS ... ok
```

通过 Nsight 我们可以看到主要 Stall 的地方是 STG.E.U16 (store global 2B)

![](https://picx.zhimg.com/v2-af42905ba2b35202dee3aa889823077f_1440w.jpg)

Stall Reason

再查一下 Transactions 数量，我们使用 l1tex\_\_t\_sectors\_pipe\_lsu\_mem\_global\_op\_ld.sum 这个标识：

![](https://picx.zhimg.com/v2-381f83d0bee69804eeabf7b9d178471f_1440w.jpg)

-   0: Float1, Load Transaction Unit Size = 1024^3 B / 33554432 = 32B
-   1: Float2, Load Transaction Unit Size = 1024^3 B / 33554432 = 32B
-   2: Float4, Load Transaction Unit Size = 1024^3 B / 33554432 = 32B
-   3: Float8, Load Transaction Unit Size = 1024^3 B / 67108864 = 16B （例外，每个线程要18个寄存器）
-   4: half, Load Transaction Unit Size = 1024^3 B / 33554432 = 32B

如上，2B / 4B / 8B / 16B Access 都是 32B Transaction 位宽。可见传输次数 只取决于 数据量 而不是 VecSize (或者说 L/S Instruction Type)，因此可以合理推测，他们都使用固定的传输大小 32B/LSU -> T-Stage，而只有 2B Access 在一个 Cycle 下只有一半的 LSU 利用率。

综上，对于访存优化，需要满足以下要求：

1.  最好使用 4B 以上向量化访存，即 >= 4B/Thread（但也不要太大，会增加寄存器压力）
2.  每个 Warp 访存的首地址最好是 32B 对齐
3.  每个 Warp 最好访问 >= 连续的 128B \* 正整数 的数据量，不建议过大

## 2\. Occupancy

由于 SM 中存在 4 个 Warp Scheduler，我们最好让它们时刻保持繁忙，因此，衡量一个 SM上 活跃 Warp 数量的相关指标，对于评估硬件利用效率至关重要。这个指标就是 Occupancy，即每个 SM 上活跃 Warp 数量与其所能支持的最大 Warp 数量之比。我们看到 Nsight 中有这么一句话：

_The 8.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the hardware maximum of 12_

每个 SM 的最佳 Warp 数量和硬件设计相关，我现在的设备（RTX4090）的硬件能让每个 Warp Scheduler 塞 12 个 Warp，但是现在做实验的 Kernel 根据 Nsight 计算，只能塞 8 个 Warp，那它的计算上限是 66% 的硬件设计最大值。而实际跑起来，我的 Copy Kernel 差不多只有计算上限的一半，相当于实际上塞了 4 Warp / WS：

![](https://pic4.zhimg.com/v2-6b8ed6753cb1bd87069a6936f713ab63_1440w.jpg)

Block Size 1024 Copy

由于 Copy Kernel 是访存受限的 （Memory Bound），因此低的 Occupancy 并不会降低总体 Throughput，这个 37.64% 已经足够了。那怎么计算到的这个 37% 呢？这里看 Achieved Active Warps Per SM，我们大概是 18.07，由于一个 SM 4 个 WS，那每个 Warp Scheduler 平均分到 4.5175 个 Warp，再除 12 就是 37.64%。

下面这句话总结了造成 Occupancy 降低的原因：

_The difference between calculated theoretical (66.7%) and measured achieved occupancy (37.6%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel._

一方面，由于 Warp 调度开销，会影响实际 Occupancy；另一方面，调度不均衡也会降低。

对于理论的 Occupancy 上限，它是在**运行之前就能被计算出**：如果一个 Block 很小但占用较多的 SLM 或者 Register，那一个 SM 大概率只塞的下一个 Block，但是由于它很小比如只有 2 个 Warp，SM 中会有 2 个空闲 Warp Scheduler，也会降低理论 Occupancy 上限。

**因此，如果硬要提高理论 Occupancy，答案很简单，降低 Block Size**。在上面的 Copy Kernel 中，我使用了 1024 Block Size，现在调成 256 再做实验 ，它理论上限尽然满了？！因为 一个 Block 的 Thread 减少 -> Block 的 Register 需求减少 & Warp 数也减少 -> 一个 SM 能加快塞 Block 的速度，也可能同时处理多个Block -> SM 中活跃 Warp 数量可能提升 ，因为加速 Block 调度更有利于流水线 -> 提升理论 Occupancy。

现在，Occupancy 理论上限就可以打满硬件限制，而实际 Occupancy 也会跟着提升。

![](https://pica.zhimg.com/v2-afe34b19f2ee73e6aff26ddcc27dfad4_1440w.jpg)

Block Size 256 Copy

然后我们再测一下 Bandwidth，2B Access 尽然接近了 Peak，之前的 LSU 效率问题被 High Occupancy 完全 Hide 住了，想象一下，Warp Scheduler 非常繁忙，虽然一次 LSU 只用一半，但是只要够快，看起来就像同一时刻用 4 个！

```text
1GB threads copy test ...
float1: timems:2.33354
920.27 GBPS ... ok
float2: timems:2.33789
918.557 GBPS ... ok
float4: timems:2.33398
920.094 GBPS ... ok
float8: timems:2.33168
921.003 GBPS ... ok
half: timems:2.34173
917.051 GBPS ... ok
```

当然，还有一种方法提升 2B Access 的性能，那就是在每个 Thread 里 For-Loop 来加快 LSU 的 Dispatch

```cuda
int offset = threadIdx.x;
#pragma unroll
for (int idx = 0; idx < UNROLL_FOR_LOOP; ++idx) { // For Loop
    out[offset] = in[offset]; // 2B Access
    offset += blockDim.x;
}
```

## 3\. Tail Effect

在 GPU 中，能真正同时运行的 Thread Block 数量称为 [Wave](https://link.zhihu.com/?target=https%3A//docs.nvidia.com/nsight-compute/ProfilingGuide/index.html) ，如果 Block 数超过 Wave 数就会被 Schedule 到下一个 Wave Step 执行。Nsight 会留一个指标叫做 Waves Per SM，如果它不是整数，比如1.5，那么由于 Block 无法被 SM 分割，SM 中三分之一资源被浪费，这就是 Tail Effect，所以它最好是整数。

## 4\. Reduction 最佳实践： Warp Shuffle

由于 Reduce 的计算访存比不超过 1，它是妥妥的访存受限 Kernel，因此我们的目标是打满 GPU Device Memory Bandwidth。我们可以进一步把它分成 Thread Reduce / Warp Reduce / Block Reduce 三个部份，简单起见，Thread Reduce 使用 for-loop 实现，Warp Reduce 使用 SHFL 指令，而 Block Reduce 使用 Atomic 指令，再引入向量化机制，核心代码如下: （[完整代码](https://link.zhihu.com/?target=https%3A//github.com/xytpai/nvbench/blob/master/standard/perf_atomic_reduce.cu)）

```cuda
namespace reduce_utils {

template <typename T, int vec_size>
struct alignas(sizeof(T) * vec_size) aligned_array {
    T val[vec_size];
};

template <typename T, typename func_t>
__device__ T warp_reduce(T val, func_t fn) {
#pragma unroll
    for (int offset = (32 >> 1); offset > 0; offset >>= 1) {
        // Warp Shuffle
        val = fn(val, __shfl_down_sync(0xffffffff, val, offset, 32));
    }
    return val;
}

template <typename T, typename func_t>
__device__ void block_reduce(T &val, T *shared, func_t fn) {
    val = warp_reduce<T, func_t>(val, fn);
    auto warp_id = threadIdx.x / 32;
    auto warp_tid = threadIdx.x % 32;
    if (warp_id == 0) {
        shared[warp_tid] = val;
    }
    __syncthreads();
    if (warp_id == 0 && warp_tid == 0) {
        for (int i = 1; i < blockDim.x / 32; ++i) {
            val = fn(val, shared[i]);
        }
    }
}

template <typename T, int vec_size, typename func_t>
__device__ T thread_reduce(T *block_start, int n, func_t fn, T ident) {
    using vec_t = aligned_array<T, vec_size>;
    T acc = ident;
    for (int i = threadIdx.x * vec_size; i < n; i += blockDim.x * vec_size) {
        auto input_vec = *reinterpret_cast<vec_t *>(&block_start[i]);
        for (int j = 0; j < vec_size; ++j) {
            acc = fn(acc, input_vec.val[j]);
        }
    }
    return acc;
}

} // namespace reduce_utils

template <typename T, int vec_size, int loops>
__global__ void reduce_kernel(T *in_, T *out, size_t reduce_size) {
    auto block_work_size = loops * blockDim.x * vec_size;
    auto block_offset = blockIdx.x * block_work_size;
    auto in = in_ + block_offset;

    __shared__ T shared[32];

    auto remaining = ::min((int)(reduce_size - block_offset), block_work_size);
    auto sum_fn = [](T a, T b) { return a + b; };
    T acc = reduce_utils::thread_reduce<T, vec_size>(in, remaining, sum_fn, 0);
    reduce_utils::block_reduce<T>(acc, shared, sum_fn);

    if (threadIdx.x == 0) {
        atomicAdd(out, acc);
    }
}
```

我们来测试下 Float4 向量化，UNROLL\_FOR 为 4 的 1GB Reduction 能达到的Throughput，可以看到已经接近 Copy 的带宽：

```text
1GB reduce test ...
float4: timems:1.15466
929.924 GBPS ... ref: 1.34218e+08, out_cuda: 1.34218e+08
```

网上有各种资料介绍 [Warp Shuffle](https://link.zhihu.com/?target=https%3A//developer.nvidia.com/blog/using-cuda-warp-level-primitives/)，这里就不多赘述，下图为 Warp Size 为 8 时的 Shuffle Reduce 操作，Warp 内的线程能看到相邻线程的值，这样就能做 Tree Reduce：

![](https://pica.zhimg.com/v2-5aaea3419007743c82ec2156efb153a6_1440w.jpg)

## 5\. 如何手撕 GEMM 跑满 Tensor Core 理论 FLOPS

好，现在看看如何手写 GEMM 跑满 Tensor Core 的理论 FLOPS，简单起见，我们会用到 WMMA 指令而不是使用 CUTLASS 这种已经完善的库，我们会使用最简单的计算受限型 Shape 比如 m = n = k = 8192 来测试性能，并对比 CuBlas 的实现方案。

标准的 Tiling 操作我们可以通过 [https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/](https://link.zhihu.com/?target=https%3A//docs.nvidia.com/cuda/cuda-c-best-practices-guide/) 来快速学习到，这里直接贴一个带 AB Buffer 的 Tiling SGEMM 实现：

```text
template <int TILE_SIZE = 32>
__global__ void sgemm_cuda_kernel(
    float *out, float *a, float *b, int m, int n, int k,
    float alpha, float beta) {
    auto tx = threadIdx.x;
    auto ty = threadIdx.y;
    auto bx = blockIdx.x;
    auto by = blockIdx.y;

    float acc = 0;

    auto tile_m = by * TILE_SIZE;
    auto tile_n = bx * TILE_SIZE;
    auto y = tile_m + ty;
    auto x = tile_n + tx;

    int write_stage_idx = 0;
    int read_stage_idx = 1;

    __shared__ float shared_a[2][TILE_SIZE][TILE_SIZE];
    __shared__ float shared_b[2][TILE_SIZE][TILE_SIZE];

    for (int tile_k = 0; tile_k < k; tile_k += TILE_SIZE) {
        float a_val, b_val;
        int k_ = tile_k + tx;

        if (y < m && k_ < k) {
            a_val = a[y * k + k_];
        } else {
            a_val = 0;
        }
        shared_a[write_stage_idx][ty][tx] = a_val;
        // no bank-conflict: a warp store contig-128B

        k_ = tile_k + ty;
        if (x < n && k_ < k) {
            b_val = b[k_ * n + x];
        } else {
            b_val = 0;
        }
        shared_b[write_stage_idx][ty][tx] = b_val;
        // no bank-conflict: a warp store contig-128B

        write_stage_idx ^= 1;
        read_stage_idx ^= 1;
        __syncthreads();

        for (int tki = 0; tki < TILE_SIZE; ++tki) {
            // no bank-conflict: half warp fetch contig-64B, half warp fetch the same contig-64B (broadcast)
            acc += shared_a[read_stage_idx][ty][tki] * shared_b[read_stage_idx][tki][tx];
        }
        
        // __syncthreads(); // 由于AB Buffer存在，我们可以去掉它
    }

    if (y < m && x < n)
        out[y * n + x] = alpha * acc + beta * out[y * n + x];
}
```

以上代码中，我们使用双缓冲来省掉了最后一个 Block Sync。我们还有一次 SLM 写操作和一次 SLM 读操作。写操作时，由于每个 Warp 的 32 个线程写连续 128B 的数据，每个线程都写不同的 Bank，因此不存在 Bank-Conflict；读操作时，一半的 Warp 读连续的 64B 数据，另一半的 Warp 读相同的 64B 数据，由于两半 Warp 读的是相同的数据，因此可被 Broadcast，所以也不会有 Bank Conflict 问题。（如果 Warp 涉及了同个 Bank 的不同行的数据读取，才会出现 Bank Conflict 问题）

下面我们在此基础上增加 WMMA 来操作 Tensor Core，先查一下我这台 4090 的性能：

![](https://pic3.zhimg.com/v2-eb480bb220a4c7815d4e0afa22f3680c_1440w.jpg)

ADA ARCH Tensor Core

我们使用 FP32 作为 Accumulate Type，可以看到 165 是峰值性能（不要看 330 这是 Sparse 性能）。接下来带上以上各种优化写出核心代码：（我会持续更新让它更简洁）

```text
template <typename scalar_t,
          int BLOCK_M_LANES, int BLOCK_N_LANES,
          int LANE_M_WARPS, int LANE_N_WARPS,
          int WARP_M_THREADS, int WARP_N_THREADS,
          int VEC_M, int VEC_N,
          int PAD = 8>
__global__ __launch_bounds__(256) void gemm_cuda_kernel(
    scalar_t *__restrict__ out, // __restrict__ 告诉编译器不会有重叠内存，方便优化
    const scalar_t *__restrict__ a,
    const scalar_t *__restrict__ b,
    const int m, const int n, const int k,
    const scalar_t alpha,
    const scalar_t beta) {
    constexpr int BLOCK_K = 32;
    static_assert(LANE_M_WARPS * LANE_N_WARPS == 8); // 使用 256 线程, 8 Warps
    static_assert(WARP_M_THREADS * WARP_N_THREADS == 32);
    constexpr int WARP_M = WARP_M_THREADS * VEC_M;
    constexpr int WARP_N = WARP_N_THREADS * VEC_N;
    static_assert(WARP_M == 16); // 每个 Warp 强制处理 16x16 的方块
    static_assert(WARP_N == 16);
    constexpr int LANE_M = LANE_M_WARPS * WARP_M;
    constexpr int LANE_N = LANE_N_WARPS * WARP_N;
    constexpr int BLOCK_M = BLOCK_M_LANES * LANE_M; // Block Tile 行数
    constexpr int BLOCK_N = BLOCK_N_LANES * LANE_N; // Block Tile 列数

    // idx
    auto tid = threadIdx.x;
    auto wid = tid >> 5;
    // auto w_tid = tid & 31;
    auto block_y = blockIdx.y;
    auto block_x = blockIdx.z * gridDim.x + blockIdx.x;

    // slm
    __shared__ scalar_t as[2][BLOCK_M * (BLOCK_K + PAD)];
    __shared__ scalar_t bs[2][BLOCK_K * (BLOCK_N + PAD)];

    nvcuda::wmma::fragment<nvcuda::wmma::matrix_a, 16, 16, 16, scalar_t, nvcuda::wmma::row_major> a_frag[2][BLOCK_M_LANES];
    nvcuda::wmma::fragment<nvcuda::wmma::matrix_b, 16, 16, 16, scalar_t, nvcuda::wmma::row_major> b_frag[2][BLOCK_N_LANES];
    nvcuda::wmma::fragment<nvcuda::wmma::accumulator, 16, 16, 16, float> o_frag[BLOCK_M_LANES][BLOCK_N_LANES];
#pragma unroll
    for (int i = 0; i < BLOCK_M_LANES; i++) {
#pragma unroll
        for (int j = 0; j < BLOCK_N_LANES; j++) {
            nvcuda::wmma::fill_fragment(o_frag[i][j], 0.0);
        }
    }

    constexpr int LDG_VEC_SIZE = 8;
    using ldg_vec_t = aligned_array<scalar_t, LDG_VEC_SIZE>;
    constexpr int LDG_A_X_THREADS = BLOCK_K / LDG_VEC_SIZE;
    constexpr int LDG_B_X_THREADS = BLOCK_N / LDG_VEC_SIZE;
    auto ldg_a_vec_idx = tid % LDG_A_X_THREADS;
    auto ldg_b_vec_idx = tid % LDG_B_X_THREADS;
    constexpr int LDG_REG_A_COUNT = BLOCK_M * BLOCK_K / LDG_VEC_SIZE / 256;
    constexpr int LDG_REG_B_COUNT = BLOCK_K * BLOCK_N / LDG_VEC_SIZE / 256;
    static_assert(LDG_REG_A_COUNT >= 1 && LDG_REG_B_COUNT >= 1);

    int write_stage_idx = 0;
    int read_stage_idx = 1;

    for (
        int a_begin = block_y * BLOCK_M * k, b_begin = block_x * BLOCK_N;
        a_begin < block_y * BLOCK_M * k + k;
        a_begin += BLOCK_K, b_begin += BLOCK_K * n) {
        {
            ldg_vec_t ldg_a_reg[LDG_REG_A_COUNT];
            ldg_vec_t ldg_b_reg[LDG_REG_B_COUNT];
#pragma unroll
            for (int i = 0; i < LDG_REG_A_COUNT; i++) {
                auto idx = 256 * i + tid;
                ldg_a_reg[i] = reinterpret_cast<ldg_vec_t *>(const_cast<scalar_t *>(a) + a_begin + (idx / LDG_A_X_THREADS) * k)[ldg_a_vec_idx];
            }
#pragma unroll
            for (int i = 0; i < LDG_REG_B_COUNT; i++) {
                auto idx = 256 * i + tid;
                ldg_b_reg[i] = reinterpret_cast<ldg_vec_t *>(const_cast<scalar_t *>(b) + b_begin + (idx / LDG_B_X_THREADS) * n)[ldg_b_vec_idx];
            }
            auto as_vec = reinterpret_cast<ldg_vec_t *>(as[write_stage_idx]);
            auto bs_vec = reinterpret_cast<ldg_vec_t *>(bs[write_stage_idx]);
#pragma unroll
            for (int i = 0; i < LDG_REG_A_COUNT; i++) {
                int y = (256 * i + tid) / LDG_A_X_THREADS;
                as_vec[y * ((BLOCK_K + PAD) / LDG_VEC_SIZE) + ldg_a_vec_idx] = ldg_a_reg[i];
            }
#pragma unroll
            for (int i = 0; i < LDG_REG_B_COUNT; i++) {
                int y = (256 * i + tid) / LDG_B_X_THREADS;
                bs_vec[y * ((BLOCK_N + PAD) / LDG_VEC_SIZE) + ldg_b_vec_idx] = ldg_b_reg[i];
            }
            read_stage_idx ^= 1;
            write_stage_idx ^= 1;
            __syncthreads();
        }

        {
            auto a_ptr = as[read_stage_idx];
            auto b_ptr = bs[read_stage_idx];
            auto warp_y = wid / LANE_N_WARPS * WARP_M;
            auto warp_x = wid % LANE_N_WARPS * WARP_N;

#pragma unroll
            for (int i = 0; i < BLOCK_M_LANES; i++) {
                auto y = i * LANE_M + warp_y;
                nvcuda::wmma::load_matrix_sync(a_frag[0][i], a_ptr + y * (BLOCK_K + PAD), BLOCK_K + PAD);
                nvcuda::wmma::load_matrix_sync(a_frag[1][i], a_ptr + y * (BLOCK_K + PAD) + 16, BLOCK_K + PAD);
            }
#pragma unroll
            for (int j = 0; j < BLOCK_N_LANES; j++) {
                auto x = j * LANE_N + warp_x;
                nvcuda::wmma::load_matrix_sync(b_frag[0][j], b_ptr + x, BLOCK_N + PAD);
                nvcuda::wmma::load_matrix_sync(b_frag[1][j], b_ptr + x + 16 * (BLOCK_N + PAD), BLOCK_N + PAD);
            }
#pragma unroll
            for (int i = 0; i < BLOCK_M_LANES; i++) {
#pragma unroll
                for (int j = 0; j < BLOCK_N_LANES; j++) {
                    nvcuda::wmma::mma_sync(o_frag[i][j], a_frag[0][i], b_frag[0][j], o_frag[i][j]);
                    nvcuda::wmma::mma_sync(o_frag[i][j], a_frag[1][i], b_frag[1][j], o_frag[i][j]);
                }
            }
        }
    }

    { // write back
        auto out_warp_y = block_y * BLOCK_M + wid / LANE_N_WARPS * WARP_M;
        auto out_warp_x = block_x * BLOCK_N + wid % LANE_N_WARPS * WARP_N;
#pragma unroll
        for (int i = 0; i < BLOCK_M_LANES; i++) {
#pragma unroll
            for (int j = 0; j < BLOCK_N_LANES; j++) {
                auto y = out_warp_y + i * LANE_M;
                auto x = out_warp_x + j * LANE_N;
                if (y < m && x < n) {
                    auto out_offset = y * n + x;
                    nvcuda::wmma::fragment<nvcuda::wmma::accumulator, 16, 16, 16, scalar_t> c_frag;
                    nvcuda::wmma::load_matrix_sync(c_frag, out + out_offset, n, nvcuda::wmma::mem_row_major);
                    for (int k = 0; k < c_frag.num_elements; k++) {
                        c_frag.x[k] = alpha * (scalar_t)o_frag[i][j].x[k] + beta * c_frag.x[k];
                    }
                    nvcuda::wmma::store_matrix_sync(out + out_offset, c_frag, n, nvcuda::wmma::mem_row_major);
                    __syncthreads();
                }
            }
        }
    }
}
```

测一下吞吐量，几乎是满格：

```text
m=8192, n=8192, k=8192, alpha=0.5, beta=0.5
6.6601 ms, 80.6101 gbps, 165.089 tflops
```