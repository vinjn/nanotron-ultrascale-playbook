# CS336学习记录-Lec3&4

**Author:** zhuobai

**Date:** 2025-07-16

**Link:** https://zhuanlan.zhihu.com/p/1928139553484563818

这两节课程学习下来，给我一种在读一篇很新的 [LLM](https://zhida.zhihu.com/search?content_id=260330557&content_type=Article&match_order=1&q=LLM&zhida_source=entity)综述 的感觉。Tatsu H 老师在做课件的时候应该是做了大量的调研工作，三个小时的时间涵盖了巨量的有用的信息。本来计划学习下 percy 老师推荐的 illustred Transformer 课程的，但又觉得整个学习过程不能中断，所以简单重温了下李宏毅老师的 transformer 系列课后就直接开始继续学习了，那话不多说，让我们开始！

## Lecture3-Architectures Hyperparameters

这堂课的主题：**_the best way to learn is hands-on experience the second best way is to try to learn from others’experience！_**（最好的学习方式是亲身实践；第二好的方式是尽量从他人的经验中学习！）

现在大模型的迭代速度非常迅速，仅仅在去年，就有19个新的密集模型发布，很多模型都对架构有一些改动。那 Tatsu H 这位老师统计了从 2017 年到 2025 年的很多经典模型对架构的具体改动情况，这节课主要从模型结构和超参数的变体出发，学习三件事情：这些模型在哪些地方具有共性？又在哪些地方存在不同？我们能从该统计表格学到些什么？

![](https://pica.zhimg.com/v2-c6277f74b113ab51dcf94953fcd2dd66_1440w.jpg)

Tatsu H 老师总结的统计表格

我们先从模型架构开始。

-   Pre vs post normalization

将 post norm 换成 pre norm 在 2024 年以前基本算是大家都认可的改法了，几乎所有的现代的 LLMs 都使用 pre norm（BERT 除外）。

![](https://pic2.zhimg.com/v2-d3aa0fdf7aaf12fe89453ac5244079ab_1440w.jpg)

左侧是经典的2017《attention is all you need》的 layer norm 示意图

那为什么这种改法受到大家的认可呢，因为采用 pre-norm 能使整个模型的训练更稳定（如下图），并且可以让梯度直接回传到原始输入，不经过 [LayerNorm](https://zhida.zhihu.com/search?content_id=260330557&content_type=Article&match_order=1&q=LayerNorm&zhida_source=entity) 的干扰。

![](https://pic3.zhimg.com/v2-6c1de988a881a3e0b1d63acd665b5d4e_1440w.jpg)

红线的收敛性明显要更好（这个在很多实验都已经被证明）

现在还有一些变体比如 double norm，在 pre-norm 的基础上再加一层 norm 不放在残差流中（不是 post-norm），目前已经有 Grok, Gemma 2 等模型采用这种做法，并且相关实验表明效果更好，但还没有大规模被应用。

-   LayerNorm vs [RMSNorm](https://zhida.zhihu.com/search?content_id=260330557&content_type=Article&match_order=1&q=RMSNorm&zhida_source=entity)

初始版的 Transformer 使用 LayerNorm ，需要对均值和方差进行归一化，代表模型有 GPT3/2/1, OPT, GPT-J, BLOOM。目前大多的 LLM 都采用 RMSNorm，不会减去均值或添加偏差项，代表模型有 LLaMA-family, PaLM, Chinchilla, T5。

那为什么选择 RMSNorm 呢，普遍的解释是更快，并且效果一样好。一方面由于不计算均值能减少计算操作，另一方面由于不需要存储 bias 所有参数量更少，但这种解释真的合理吗？在下面这个表格中我们可以看到normalization 只占到 0.17% 的 flop，那应该对计算速度的影响是微乎其微的，但是我们忽略了一件事情：**_data movement，_**由于内存转移的原因，norm 实际花费的时间占用到了25.5%，所以理论来说这种改进是有效的，通过表格我们也可以看出 2022 年后大家基本都用 RMSNorm 了。

![](https://pic1.zhimg.com/v2-1e8caf46dcb059331f6188aa2089464a_1440w.jpg)

各环节计算速度与运行时间统计表格

-   Activations

首先说下为什么要在深度学习中使用 activation，我在《dive into deep learning》的过程中对这个问题的解释印象特别深刻，那就是如果没有激活函数（即所有层都是线性变换），整个 Transformer 就只是一个**线性映射**，再深也无法表征复杂函数。

初始版的 Transformer 使用 ReLU 作为激活函数，后续 GPT1/2/3, GPTJ 等模型开始使用 GeLU，从2023年开始，很多模型都开始使用 [Gated activations](https://zhida.zhihu.com/search?content_id=260330557&content_type=Article&match_order=1&q=Gated+activations&zhida_source=entity) 的变体 SwiGLU（Swish-Gated Linear Unit）。如下图所示，在基础的 GLU 上加了层 swish ，也就是 ∗ sigmoid( )，引入 V 作为一个可学习的参数矩阵，⊗ 表示 **逐元素乘法（element-wise product），**体现了“门控”思想。

![](https://pic1.zhimg.com/v2-5225fe835c363bc02f2c6e899e18ac12_1440w.jpg)

SwiGLU公式

-   Serial vs Parallel layers

传统的 Transformer Block 都是连续的，在这几年也有工作在尝试将 block 并行化计算，比如模型 GPT-J，但整体来看几乎没有其他模型采用这种方式。

-   Many variations in [position embeddings](https://zhida.zhihu.com/search?content_id=260330557&content_type=Article&match_order=1&q=position+embeddings&zhida_source=entity)

虽然 position embeddings 不属于架构这部分，但为了文章结构清晰点我还是放在架构变体这里了。初版的 Transformer 使用 Sine embeddings，GPT1/2/3, OPT 等模型使用 Absolute embeddings（只需要加一个位置向量），谷歌的一些模型比如 T5 使用 Relative embeddings，那 2024 年以后大多数模型比如 GPTJ, PaLM, LLaMA 都开始使用 Rope embeddings（rotary position embeddings）了，那我们看看这到底是个什么东西。

在 RoPE 之前，现有的位置编码方式无法确保注意力函数只依赖相对位置 i−j，其中 Sine embeddings 不能只由 i−j 控制，不是纯相对位置，并且没有外推性，如果预训练最大长度为512的话，那么最多就只能处理长度为512的句子，而 Relative embeddings 是加法结构，不是内积，导致表达上与注意力机制结构不自然，不具备旋转不变性。那 RoPE 就很好的解决了这些问题，采用旋转矩阵相乘的计算方式添加位置信息。具体的计算比较复杂，这里就不深入了。

![](https://pic4.zhimg.com/v2-3149e08c6aba2797aeb7f27920c94767_1440w.jpg)

RoPE 在二维空间的计算示意图

OK，模型架构就基本讲到这里了，下面是 Hyperparameters（超参数）的部分了。

-   Feedforward – model dimension ratio

输入向量 x 的维度为 model dim ( )，而 FFN 的隐藏层的维度为 feedforward dim ( ) ，它们之间通常关系为 = 4 \* ，这是科研界普遍认可的常识。目前很多模型都使用 GLU 作为激活函数，由于 GLU 引入了门控机制，它本质上把前馈网络的非线性处理压缩了一部分，**会减少一定的表示能力，**大概会压缩计算量到原来的 2/3，所以目前很多现代模型采用的参数范围大概是 2.5-3.5，主要在 8/3 附近**。**也有一些模型比如 T5 甚至尝试将该数值拉到64，并且实验结果也不差（看来总有学者想尝试些新的东西，非常 bold！）。

![](https://pica.zhimg.com/v2-0843d8f7fac652be9311f0d486cbe41a_1440w.jpg)

Transformer 的两层前馈神经网络（FFN）示意图

-   Head-dim\*num-heads to model-dim ratio

大多数模型都保持在 1 左右。

-   Aspect ratios

模型应该更宽还是更深呢，目前很多现代模型的大致比例 / r 大概在100-150左右，一般来说特别深的模型很难去并行化并且有很高的 latency，下面这张图看起来非常直观。

![](https://pic1.zhimg.com/v2-e9f345346d740093570680e026f03516_1440w.jpg)

不同的比例对模型表现的影响

最后一部分主要介绍了一些最新的训练稳定的 trick，这块没有详细记录，后面 train model 的时候可以再仔细学习下。

## Lecture4-Mixture of experts

[MoE](https://zhida.zhihu.com/search?content_id=260330557&content_type=Article&match_order=1&q=MoE&zhida_source=entity) 是目前最先进的大模型（GPT、[DeepSeek](https://zhida.zhihu.com/search?content_id=260330557&content_type=Article&match_order=1&q=DeepSeek&zhida_source=entity)，LLaMa，Grok 等）都采用的一种架构，那什么是 MoE 呢？MoE 的基本思想是使用很多个大的 FFN 和一个 select layer 去替换一个大的 FFN，如下图所示。

![](https://pic3.zhimg.com/v2-30765b32fbdee55e032d385aac6894c8_1440w.jpg)

Dense Model 和 Sparse Model 的不同

为什么现在 MoE 如此流行呢，它和 Dense Model 相比，能够达到同样的 Flops，参数量越高表现越好，训练起来速度更快，可以并行到很多设备等等。这里已经有很多实验证明了，就不再放一些图表了。

课程的核心内容主要是从三个方面讲一些 MoE 的变体。

-   Routing function

很多的 routing algorithms 都可以总结为‘choose top k’，总共有三种范式可以选择，Token chooses expert，Expert chooses token，全局 routing。那目前几乎所有的 MoEs 都选择标准的‘token choice topk’ routing。

![](https://pic1.zhimg.com/v2-8520bc5cae065894ad4c200bb9fde594_1440w.jpg)

Choose Top-K 的三种范式

关于 K 的大小的话，一般认为大于 2 就可以了，那不同模型对于 K 的设置不同，其中 Grok ( K=2 ), Qwen ( K=4 ), DeepSeek ( K=7 )。

![](https://picx.zhimg.com/v2-270d6760fb1a9fb4e0acf01cf340cb0f_1440w.jpg)

Top-2 Routing 示意图

在录制这堂课的时候，DeepSeek 已经发布了四个月多的时间了，所以这里也专门讲了下 DeepSeek V3 的架构创新性，那 DeepSeek V3 证明了参数量少，数量更多的专家模型+一个共享的专家模型（如下图）是非常有效的，能在多个Benchmark 的表现变强很多，这个可以在对应的消融实验结果看出来。

![](https://pica.zhimg.com/v2-cbe82a302924c9a883ba555ac932bf3c_1440w.jpg)

new variants 示意图

那我们应该怎么训练 MoEs 呢，主要的问题在于：为了提高训练时间效率，我们需要稀疏性…… 但 sparse gating decisions 是不可微的。solution？

1.  Reinforcment learning to optimize gating policies。RL 是‘正确的解决方案’，但由于 gradient variances 和 complexity，因此尚未被广泛应用；
2.  Stochastic perturbations。这里主要讲了这两年关于如何随机扰动的工作，比如加一些高斯噪声之类的；
3.  Heuristic balancing losses（启发式平衡损失）。系统效率要求我们均衡地使用专家模型。这里讲了 DeepSeek 从 v1 到 v3 的一些使用到的技术细节。

后面开始讲一些 MoE 的问题，比如训练时不够稳定，fine-tuning 的时候容易过拟合等等，也讲解了下最近几年工作的解决方案，感觉这里比较深入了，没有详细记录。

此外，这位老师还专门回顾了 DeepSeek MoE v1-v2-v3 的结构，Routing strategy 以及如何修改添加 loss 提升模型训练的稳定性，并且提了下 v3 的一些关键创新技术（DeepSeek 真是开源模型的超级典范！）。

最后总结下 MoE 吧！

-   MoE 利用了稀疏性——并非所有输入都需要整个模型的参与；
-   Discrete routing 很难，但 top-k 启发式方法看起来效果不错；
-   大量实证证据表明 MoE 有效且性价比高，所以现在顶级模型都基本采用这种架构。