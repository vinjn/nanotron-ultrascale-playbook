# DeepSeek-R1大模型学习笔记

**Author:** 聚丙烯酰胺

**Date:** 2025-02-05

**Link:** https://zhuanlan.zhihu.com/p/21616848689

﻿## ﻿[DeepSeek-R1](https://zhida.zhihu.com/search?content_id=253345560&content_type=Article&match_order=1&q=DeepSeek-R1&zhida_source=entity)模型架构设计

DeepSeek-R1基于[DeepSeek-V3](https://zhida.zhihu.com/search?content_id=253345560&content_type=Article&match_order=1&q=DeepSeek-V3&zhida_source=entity) base模型，提出了一系列训练策略，包括基于纯强化学习的训练（[DeepSeek-R1-Zero](https://zhida.zhihu.com/search?content_id=253345560&content_type=Article&match_order=1&q=DeepSeek-R1-Zero&zhida_source=entity)）、基于多阶段的训练和冷启动（DeepSeek-R1）、知识蒸馏等。下面是我总结的DeepSeek系列的整体框架：

![](https://pica.zhimg.com/v2-5bb86c9466789ec3c7c79deafd486bf6_1440w.jpg)

在这里插入图片描述

### 专家混合模型（[MoE](https://zhida.zhihu.com/search?content_id=253345560&content_type=Article&match_order=1&q=MoE&zhida_source=entity)）

MoE在每次推理时选择性地激活部分模型参数，在不成比例增加计算成本的情况下，可以扩展模型参数。在DeepSeek-V2中就已经提出了用于FFN层的DeepSeekMoE。 _动态专家分配：根据token的上下文动态分配合适的专家_ DeepSeek-V2引入辅助损失进行负载均衡，确保token在专家之间的分配更加均衡。DeepSeek-V3和DeepSeek-R1进一步采用用auxiliary-loss-free load balancing实现负载均衡，引入一个expert bias，这个bias只影响专家路由，而不影响任何梯度。动态调整bias，专家overloaded则降低bias，专家unoverloaded则增大bias。简单来说就是用加法高效地对gating score进行re-weight的过程 \* DeepSeek-R1和DeepSeek-V3一致，总参数量671B，通过MoE对单个token的激活参数量仅37B (~5.5%)。MoE中有1个shared expert+256个routed expert，每次只激活的8个exert。

![](https://pica.zhimg.com/v2-a3f0fa04374ec024422ca844ec425102_1440w.jpg)

**Auxiliary-Loss-Free Load Balancing** 和DeepSeek-V3一样，DeepSeek-R1采用了细粒度的MoE，一些expert作为共享expert，另一些expert作为routed expert进行动态激活。对于第t个token $u_t$，下面是MoE计算的过程：

![](https://pic1.zhimg.com/v2-6d1d72818cef3a7ef842a260a4af21e4_1440w.jpg)

以前基于auxiliary loss的方法需要修改loss function，当auxiliary loss很大时会影响模型性能。那么Auxiliary-Loss-Free则是在gating value g的基础上，额外加上了bias来实现负载均衡：

![](https://pic2.zhimg.com/v2-c74371ee1c94b778fa221d9ca62c69df_1440w.jpg)

注意bias只影响专家路由，而不影响任何梯度。专家overloaded则降低bias，专家unoverloaded则增大bias。调整的速度由超参数 $\gamma$ 控制，这个和反向传播的梯度更新过程类似。

下图是该方法的出处：Auxiliary-Loss-Free Load Balancing Strategy for Mixture-of-Experts文章所提出的负载均衡策略：

![](https://pic1.zhimg.com/v2-436903ce56583e8964fb4a320351bdaa_1440w.jpg)

和DeepSeek-V2一样，DeepSeek-V3和DeepSeek-R1都采用了限制设备数量的MoE，并且不会再训练时做token dropping了。

### 多头潜在注意力（[MLA](https://zhida.zhihu.com/search?content_id=253345560&content_type=Article&match_order=1&q=MLA&zhida_source=entity)）

MLA通过将QKV矩阵投影到低维潜在空间，显著降低计算和内存成本。DeepSeek-V2中就提出了用MLA来替代传统的多头自注意力。

MLA和其他注意力的对比如下，KV cache以一个更低的维度去存储和计算。

![](https://pic2.zhimg.com/v2-1236f85e2aff3325bbc643d916349855_1440w.jpg)

在这里插入图片描述

K和V的联合压缩如下：

![](https://pic2.zhimg.com/v2-bc354348ef95fddfb12208a06d985a47_1440w.jpg)

真正推理时，cache的就是低维的 $c_t^{KV}$ ，并且down-proj和up-proj矩阵可以分别被吸收进 $W^Q$ 和 $W^O$ 中，不会造成额外的计算开销。这个方法和Palu: Compressing KV-Cache with Low-Rank Projection那篇文章一致。具体的融合过程如下（以 $ W^Q $ 的融合为例）：

![](https://pic4.zhimg.com/v2-6973b6a91922a24a3d1f6473188da091_1440w.jpg)

为了在训练时降低激活的memory，也对query做低秩压缩：

![](https://pic1.zhimg.com/v2-0e41dfd61db456527d9444facb287d26_1440w.jpg)

【还没理解到对query低秩分解怎么省计算，算的时候不需要重构回去？】

**[RoPE](https://zhida.zhihu.com/search?content_id=253345560&content_type=Article&match_order=1&q=RoPE&zhida_source=entity)位置编码兼容性考虑** 但是KV cache的低秩压缩和RoPE位置编码并不兼容！如果对 $k_t^C$ 做RoPE， $W^{UK}$ 会和位置敏感的RoPE矩阵耦合在一起，从而不能在推理时被吸收进 $W^Q$ 中（这里应该强调一下吸收是totally offline完成的），带来额外的计算。 进一步理解 $W^{UK}$ 和RoPE矩阵的耦合：与生成当前token相关的RoPE矩阵位于 $W^{Q}$ 和 $W^{UK}$ 之间，而矩阵乘法不满足交换律。 于是DeepSeek-V2提出了解耦RoPE策略，用额外的多头query和一个共享key来计算RoPE，然后和原本的query和key拼接起来。至于这里怎么得到的额外query和key，就是用来两个额外的线性层来算得的。

![](https://pic1.zhimg.com/v2-1768fc9edc2215200a98055b7810ca68_1440w.jpg)

下图体现了MLA的整个过程，值得注意的一点是，MLA的低秩分解是基于训练的，而非用SVD之类的方式post-training分解直接推理（比如Pula文章）。

![](https://pica.zhimg.com/v2-78fc5a4d970cf18b9d20a56d7df726be_1440w.jpg)

## 训练策略

### DeepSeek-R1-Zero with RL only

![](https://picx.zhimg.com/v2-270b1f1d8a061fae3d4119f4902d7add_1440w.jpg)

DeepSeek-R1-Zero直接在DeepSeek-V3 base模型的基础上用纯的Group Relative Policy Optimization (GRPO)强化学习算法，而不引入Supervised Fine-tuning ([SFT](https://zhida.zhihu.com/search?content_id=253345560&content_type=Article&match_order=1&q=SFT&zhida_source=entity))训练。

DeepSeek-R1-Zero采用基于规则的奖励机制，包含1）accuracy奖励和2）格式奖励，模板如下图所示，思考过程和回答过程需要放在对应的tag中。

![](https://pica.zhimg.com/v2-3b2250be733c1bea1b17982ce2f76e3a_1440w.jpg)

**DeepSeek-R1-Zero的缺点** DeepSeek-R1-Zero面临着可读性差和语言混合（比如中英文混杂）等挑战。

### DeepSeek-R1 with Both RL and SFT

在DeepSeek-R1-Zero的基础上，DeepSeek-R1加入了冷启动，并且用高质量数据做SFT+RL训练，得到了当今的“最强大模型”。下面是关键技术和训练流程：

**步骤1：冷启动SFT**

在高质量的长思维链（CoT）数据上做SFT，改善DeepSeek-R1-Zero可读性较差的问题。数据的采集方式如下：以带长CoT的few-shot prompting为例，直接提示模型生成带思考和验证的详细的答案；以可读的格式收集DeepSeek-R1-Zero的输出，并通过人工细化和调整结果。

**步骤2：面向推理的RL**

冷启动训练后，用和DeepSeek-R1-Zero一样的RL训练策略继续训练，来提高模型的推理能力。训练过程中发现CoT经常表现出语言混合，特别是当RL提示涉及多种语言时，所以提出采用语言一致性奖励改善语言混杂的问题，进一步增强模型的可读性。

**步骤3：拒绝采样和SFT**

RL训练收敛后，在这一步中融合推理数据和非推理数据进一步微调模型。

-   对于推理数据，用上述RL训练的模型checkpoint中通过拒绝采样生成600k条推理数据。具体做法是借助生成式奖励模型，将ground-truth和模型预测结果输入DeepSeek-V3进行评估。此外，由于模型输出有时是混乱的，难以阅读，所以过滤了混合语言、长段落和代码块的CoT。

> 拒绝采样微调（RFT）：在一个微调过的模型上进行多个样本的采样，我们有一个拒绝或者接受函数来对模型采样生成的样本进行过滤筛筛选出符合我们目标分布的样本（比如预测正确的样本），再进行进一步的模型微调。

-   对于非推理数据（比如写作、事实质量保证、自我认知和翻译），采用DeepSeek-V3 pipeline，复用DeepSeek-V3的部分SFT数据集。对于某些非推理任务，调用DeepSeek-V3，在通过 prompting在回答问题之前生成一个潜在的CoT。对于很简单的query比如"Hello"，则不需要CoT了。这部分数据大约200k。

然后融合上述数据，然后进行SFT，以适应非推理场景（通用能力）。总共大约800k条数据，训练DeepSeek-V3-Base 2个epoch。

**步骤4：全场景RL**

进一步使模型与人类的偏好保持一致，进一步用了二阶段RL阶段提高模型的帮助性和无害性，同时改进其功能推理能力。对于推理任务，继续用基于规则的奖励；对于通用任务，采用偏好奖励模型。

### 多token预测（[MTP](https://zhida.zhihu.com/search?content_id=253345560&content_type=Article&match_order=1&q=MTP&zhida_source=entity)）

MTP使DeepSeek-R1并行预测多个token，从而显著提高推理速度。MTP已经在DeepSeek-V3中已经被用于训练的目标。 _并行解码：通过允许在相同的上下文窗口内进行多个token生成预测，扩展了自回归框架_ 动态预测视距：根据模型置信度调整每步预测的token数量 _强化学习引导的token选择：确保多token预测中的一致性，并减少错误传播_ 训练时MTP包含了多个MTP模块，主要用于提升模型的推理性能，在推理时，可以直接将多个MTP模块丢弃，只保留主模型，然后推理。也可以重新利用这些MTP模块，借助speculative decoding来加速推理过程。

![](https://pic2.zhimg.com/v2-7729e4912dd1c46a2f9988f76d3b1673_1440w.jpg)

### [FP8量化](https://zhida.zhihu.com/search?content_id=253345560&content_type=Article&match_order=1&q=FP8%E9%87%8F%E5%8C%96&zhida_source=entity)

DeepSeek-R1利用8位浮点数（FP8）量化，减少内存使用和计算成本，同时保持数值稳定性。 _自适应位宽：根据计算需求动态调整不同层的位宽精度_ 感知损失的量化：使用损失敏感的缩放函数，确保在不同计算阶段保持数值精度

### 知识蒸馏

用DeepSeek-R1直接蒸馏Qwen2.5-Math-1.5B, Qwen2.5-Math-7B, Qwen2.5-14B, Qwen2.5-32B, Llama-3.1-8B和Llama-3.3-70B-Instruct，都有显著的性能提升。蒸馏时只在800k个样本上用了SFT，而并没有用RL。直接说一下文章的结论： _将更强大的模型蒸馏成更小的模型会产生良好的结果，而依赖于大规模RL的小模型则需要巨大的算力，甚至可能无法达到蒸馏的性能。_ 虽然蒸馏既经济又有效，但超越智能的边界可能仍然需要更强大的基座模型和更大规模的RL。

下面是在Qwen上蒸馏和RL的对比：

![](https://pic3.zhimg.com/v2-da4fa320ebaddee2680f0d8f8060401c_1440w.jpg)