# pytorch 源码分析 （7）算子调用栈

**Author:** 阿嚏

**Date:** 2025-07-01

**Link:** https://zhuanlan.zhihu.com/p/1923379408162517147

本篇分析[pytorch](https://zhida.zhihu.com/search?content_id=259771828&content_type=Article&match_order=1&q=pytorch&zhida_source=entity)算子的实现调用栈。

pytorch主要是由python和底层c++实现的，那么如何更改底层的算子，并在顶层调用到是个问题，这种大型代码库往往有很多自动生成代码的逻辑，所以很难光靠看代码分析。

以softmax为例，其最底层真正干活的kernel函数其中一个为cunn\_SoftMaxForward，这个函数是通过c++代码host\_softmax调用到的。

而host\_softmax是被void structured\_softmax\_cuda\_out::impl 这个实现调用的

```text
#define TORCH_IMPL_FUNC(name) void structured_##name::impl


TORCH_IMPL_FUNC(softmax_cuda_out) (
  const Tensor &input,
  const int64_t dim,
  const bool half_to_float,
  const Tensor &output) {
  host_softmax<SoftMaxForwardEpilogue,false>(input, dim, half_to_float, output);
}
```

而这个softmax\_cuda\_out，则是写到一个yaml文件native\_functions.yaml中，Pytorch在编译时，会自动根据这个文件生成很多代码。关于这个文件的解释，在pytorch/aten/src/[ATen](https://zhida.zhihu.com/search?content_id=259771828&content_type=Article&match_order=1&q=ATen&zhida_source=entity)/native/README.md中。

```text
- func: softmax.int(Tensor self, int dim, ScalarType? dtype=None) -> Tensor
  variants: function, method

- func: softmax.int_out(Tensor self, int dim, ScalarType? dtype=None, *, Tensor(a!) out) -> Tensor(a!)
  variants: function
  dispatch:
    CompositeExplicitAutograd: softmax_out

- func: softmax.Dimname(Tensor self, Dimname dim, *, ScalarType? dtype=None) -> Tensor
  variants: function, method

- func: _softmax(Tensor self, int dim, bool half_to_float) -> Tensor
  structured_delegate: _softmax.out
  dispatch:
    MkldnnCPU: mkldnn_softmax
    NestedTensorCPU, NestedTensorCUDA: softmax_nested
  tags: core

- func: _softmax.out(Tensor self, int dim, bool half_to_float, *, Tensor(a!) out) -> Tensor(a!)
  structured: True
  dispatch:
    CPU: softmax_cpu_out
    CUDA: softmax_cuda_out
    MPS: softmax_mps_out
```

即便不看文档，我们大体也可以猜测出来，pytorch根据分发设备的不同，实现了三个函数softmax\_cuda\_out softmax\_cpu\_out等，这三个函数的参数，都是\_softmax.out对应的。

同时，我们注意到，softmax有两个，一个是softmax，一个是\_softmax 这俩是不一样的，也就是实现了两个底层函数。

同时，其都有很多实现，后面带个. 就是重载的意思，不过.out相对比较特殊。

有了这两个yaml后，pytorch编译的时候，就会自动生成很多代码，这些代码在pytorch/build/aten/src/ATen/ops/softmax.h和pytorch/build/aten/src/ATen/ops/\_softmax.h 下面，分别对应softmax和\_softmax。

我们以softmax为例，我们发现其实现了四个函数，除了softmax\_out多了个函数名，像softmax.Dimname就是softmax的重载函数，只是传参不一样，我们注意到其实还生成了softmax\_Dimname这种函数，其中有一个call方法比较核心，具体的实现在pytorch/build/aten/src/ATen/Operators\_3.cpp下面。

```text
// aten::softmax.int(Tensor self, int dim, ScalarType? dtype=None) -> Tensor
inline at::Tensor softmax(const at::Tensor & self, int64_t dim, ::std::optional<at::ScalarType> dtype=::std::nullopt) {
    return at::_ops::softmax_int::call(self, dim, dtype);
}

// aten::softmax.int_out(Tensor self, int dim, ScalarType? dtype=None, *, Tensor(a!) out) -> Tensor(a!)
inline at::Tensor & softmax_out(at::Tensor & out, const at::Tensor & self, int64_t dim, ::std::optional<at::ScalarType> dtype=::std::nullopt) {
    return at::_ops::softmax_int_out::call(self, dim, dtype, out);
}
// aten::softmax.int_out(Tensor self, int dim, ScalarType? dtype=None, *, Tensor(a!) out) -> Tensor(a!)
inline at::Tensor & softmax_outf(const at::Tensor & self, int64_t dim, ::std::optional<at::ScalarType> dtype) {
    return at::_ops::softmax_int_out::call(self, dim, dtype, out);
}

// aten::softmax.Dimname(Tensor self, Dimname dim, *, ScalarType? dtype=None) -> Tensor
inline at::Tensor softmax(const at::Tensor & self, at::Dimname dim, ::std::optional<at::ScalarType> dtype=::std::nullopt) {
    return at::_ops::softmax_Dimname::call(self, dim, dtype);
}
```

不考虑 softmax\_Dimname这种底层函数，这个稍微顶层的softmax是在哪里调用的呢？是在pytorch/aten/src/ATen/native/SoftMax.cpp，这个就不是生成的代码了，而是pytorch自己写的，这里举两个例子：

看到同样是softmax，因为参数的不同，重载到不同的底层实现。

其实对于底层的at::softmax和at::\_softmax，其主要意思不同在于一个是外部一些，一个内部一些，主要差异在half\_to\_float这个参数上。

```text
Tensor softmax(const Tensor& self, Dimname dim, optional<ScalarType> dtype, std::optional<at::Tensor> const& the_max_values_opt) {
  return at::softmax(self, dimname_to_position(self, dim), dtype,the_max_values_opt);
}

Tensor softmax(const Tensor& input_, const int64_t dim_, std::optional<ScalarType> dtype, std::optional<at::Tensor> const& the_max_values_opt) {
  //std::cout << 111111112 << std::endl;
  auto result = [&]() {
    NoNamesGuard guard;
    if (input_.is_cuda() && input_.scalar_type() == ScalarType::Half && dtype == ScalarType::Float){
        return at::_softmax(input_, dim_, true, the_max_values_opt);
    } else {
        Tensor converted = dtype.has_value() ? input_.toType(dtype.value()) : input_;
        return at::_softmax(converted, dim_, false, the_max_values_opt);
    }
  }();
  namedinference::propagate_names(result, input_);
  return result;
}
```

那么如何从torch.softmax等调用这个方法呢，发现pytorch又生成了pytorch/torch/\_C/\_VariableFunctions.pyi这个文件，具体生成方法不详，但看着也与那个yaml有关，猜测应该是根据yaml,即生成顶层的python接口，也生成底层的c++实现。

所以调用步骤为：

1 顶层python生成接口 （pytorch/torch/\_C/\_VariableFunctions.pyi）

2 -> 中间自己写好的代码 （pytorch/aten/src/ATen/native/SoftMax.cpp）这个是一个承上启下的作用，例如 顶层只是调用softmax,但是底层可能需要调用到\_softmax

3 -> 底层生成的代码（pytorch/build/aten/src/ATen/ pytorch/build/aten/src/ATen/ops/）

4 -> 真正的kernel代码 （pytorch/aten/src/ATen/native/cuda/SoftMax.cu）

其中1和3根据yaml生成代码，他们的参数，函数名都是对应的。

```text
@overload
def softmax(input: Tensor, dim: _int, dtype: Optional[_dtype] = None, *, out: Optional[Tensor] = None) -> Tensor: 
 r"""
    softmax(input, dim, *, dtype=None) -> Tensor
 
    Alias for :func:`torch.nn.functional.softmax`.
    """
    ...
@overload
def softmax(input: Tensor, dim: Union[str, ellipsis, None], *, dtype: Optional[_dtype] = None) -> Tensor: 
 r"""
    softmax(input, dim, *, dtype=None) -> Tensor
 
    Alias for :func:`torch.nn.functional.softmax`.
    """
    ...
```

除了yaml外，还有一个tools/autograd/derivatives.yaml，这个是反向传播用的，参数语法和native\_functions.yaml差不多。